["IRRATIONALITY IN Al\n\n1. Idea\n\nMost Al systems aim to be perfectly rational, but human thinking is not. This project explores\nbuilding an Al that deliberately replicates human cognitive biases, offering insights into\npsychology, improving human-Al interaction, and even helping debias other models.\n\nGoal: To train a model to make \"human-like\" irrational decisions by incorporating known\ncognitive biases (e.g., confirmation bias, sunk cost fallacy, anchoring).", "Why?\ne Improve human-Al collaboration (e.g., chatbots that \"think\" like humans).\ne Test economic/psychological theories in simulated environments.\ne Stress-test other Al systems by exposing them to biased inputs.\ne Create more realistic NPCs in games/simulations.\n\n2. Key Cognitive Biases to Model\n\nBias How Al Would Mimic It\n\nConfirmation Bias Al ignores contradicting evidence, favoring data that aligns with\nits \"beliefs.\"", "Anchoring Effect Al over-relies on the first piece of information it sees (e.g., initial\nprice influences decisions).\n\nSunk Cost Fallacy Al continues investing in a losing decision because of past\n\"effort.\"\n\nGambler\u2019s Fallacy Al expects random events to \"balance out\" (e.g., \"I lost 5 times,\nso I'll win now\").\n\nDunning-Kruger Effect Al overestimates its competence in unfamiliar tasks.", "3. Technical Approach\nA. Data Collection & Bias Injection\ne Dataset:\no Human decision-making experiments (e.g., lowa Gambling Task,\nCognitive Reflection Test).\no Crowdsourced biased choices (e.g., \"Would you rather keep a losing\nstock or sell it?\").\n\ne Synthetic Data Generation:\n\no Using LLMs (like GPT-4) to simulate biased reasoning (e.g., \"Input like:\nPretend you\u2019re overconfident and answer these questions\").\n\nB. Model Architecture\ne Base Model:", "B. Model Architecture\ne Base Model:\n\no A reinforcement learning (RL) agent or transformer-based\ndecision-maker.", "e Bias Injection Methods:\no Reward Shaping: Penalizing rational decisions, rewarding biased ones.\no Attention Manipulation: Forcing the model to overweight certain inputs\n(e.g., first-seen data for anchoring).\no Memory Corruption: Simulating \"selective memory\" by dropping\ncontradicting evidence.\ne Adversarial Training:\n\u00b0 Training a \"rational\" Al to debate the \"irrational\" Al, refining biases.\nC. Evaluation Metrics\ne Bias Fidelity Score: How closely Al matches human bias benchmarks.", "e Predictive Irrationality: Does it make suboptimal choices like humans?\ne Human-likeness: User studies to see if people perceive Al as \"human-like.\"\n\n4. Potential Applications\n1. Behavioral Economics Research\n\ne Simulating how markets behave under irrational agents (e.g., stock bubbles).\ne Test nudging strategies to counteract biases.\n\n2. Gaming & NPC Design\n\ne NPCs with \"flawed\" decision-making (e.g., overconfident villains, superstitious\nallies).", "3. Al Safety & Debias Testing\ne \"Adversarial bias attacks\": Using irrational Al to expose weaknesses in other\nmodels.\ne Improving human-Al alignment by understanding irrationality.\n\n4. Psychology & Therapy Tools\n\ne \"Bias mirror\" to help people recognize their own flawed thinking.\n\n5. Challenges & Ethical Considerations", "5. Challenges & Ethical Considerations\n\ne Unintended Consequences: Could an \"irrational Al\" be harmful if deployed carelessly?\ne Data Limitations: Human biases are context-dependent; hard to generalize.\ne Ethics of \"Artificial Stupidity\": Should we deliberately make Al worse?"]